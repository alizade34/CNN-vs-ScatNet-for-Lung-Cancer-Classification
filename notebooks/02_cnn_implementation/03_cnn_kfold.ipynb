{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "GPU Memory: 8.6 GB\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "# üîÑ CNN K-Fold Cross-Validation\n",
    "Visual Intelligence Project - DeepLIFT Assignment\n",
    "Phase 1: Data & CNN Foundation (Day 4)\n",
    "\n",
    "**Objective**: Implement 5-fold cross-validation for CNN to get robust performance metrics\n",
    "**Requirements**: Mean accuracy and F1 scores, statistical analysis, model validation\n",
    "**Target**: Prove CNN consistently achieves >70% accuracy across all folds\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# üì¶ IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "import torch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Disable compilation features that cause issues\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torch')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üñ•Ô∏è  Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded from previous notebooks\n",
      "üìÅ Project Paths:\n",
      "Processed Data: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\data\\processed\n",
      "Models: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\models\n",
      "Results: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\results\n",
      "K-Fold Results: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\results\\kfold\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# üóÇÔ∏è LOAD CONFIGURATION AND SETUP PATHS\n",
    "# =============================================================================\n",
    "\n",
    "# Define paths robustly relative to the notebook location\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
    "CONFIG_PATH = PROJECT_ROOT / \"config.json\"\n",
    "\n",
    "# Load configuration from previous notebooks\n",
    "if CONFIG_PATH.exists():\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"‚úÖ Configuration loaded from previous notebooks\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Configuration not found, using default paths\")\n",
    "    config = {\n",
    "        \"dataset\": {\"processed_path\": str(PROJECT_ROOT / \"data\" / \"processed\")},\n",
    "        \"paths\": {\n",
    "            \"project_root\": str(PROJECT_ROOT),\n",
    "            \"models\": str(PROJECT_ROOT / \"models\"),\n",
    "            \"results\": str(PROJECT_ROOT / \"results\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Setup paths\n",
    "PROCESSED_DATA_PATH = Path(config[\"dataset\"][\"processed_path\"])\n",
    "MODELS_PATH = Path(config[\"paths\"][\"models\"])\n",
    "RESULTS_PATH = Path(config[\"paths\"][\"results\"])\n",
    "\n",
    "# Create K-fold specific results directory\n",
    "KFOLD_RESULTS_PATH = RESULTS_PATH / \"kfold\"\n",
    "KFOLD_RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Project Paths:\")\n",
    "print(f\"Processed Data: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"Models: {MODELS_PATH}\")\n",
    "print(f\"Results: {RESULTS_PATH}\")\n",
    "print(f\"K-Fold Results: {KFOLD_RESULTS_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# üèóÔ∏è MODEL CLASSES (IDENTICAL TO PREVIOUS NOTEBOOKS)\n",
    "# =============================================================================\n",
    "\n",
    "class SharedClassifier(nn.Module):\n",
    "    \"\"\"Shared classifier for CNN and ScatNet - identical to previous notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self, input_features, num_classes=2, dropout_rate=0.5):\n",
    "        super(SharedClassifier, self).__init__()\n",
    "        \n",
    "        self.input_features = input_features\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            nn.Linear(input_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Second fully connected layer\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Third fully connected layer\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate / 2),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "class LungCancerCNN(nn.Module):\n",
    "    \"\"\"Enhanced CNN model - identical to previous notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2, dropout_rate=0.5, input_channels=3):\n",
    "        super(LungCancerCNN, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # First convolutional block (3 -> 32)\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Second convolutional block (32 -> 64)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Third convolutional block (64 -> 128)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Fourth convolutional block (128 -> 256)\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Fifth convolutional block (256 -> 512)\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((2, 2)),\n",
    "            nn.Dropout2d(0.25),\n",
    "        )\n",
    "        \n",
    "        # Calculate feature size\n",
    "        self.feature_size = 512 * 2 * 2\n",
    "        \n",
    "        # Shared classifier\n",
    "        self.classifier = SharedClassifier(\n",
    "            input_features=self.feature_size,\n",
    "            num_classes=num_classes,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"Extract features before classification\"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# üìä ENHANCED DATASET CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class LungCancerDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset class for K-fold validation\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', transform=None, class_to_idx=None):\n",
    "        self.data_dir = Path(data_dir) / split\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.classes = ['adenocarcinoma', 'benign']\n",
    "        self.class_to_idx = class_to_idx or {'adenocarcinoma': 0, 'benign': 1}\n",
    "        self.samples = []\n",
    "        self.class_counts = {cls: 0 for cls in self.classes}\n",
    "        \n",
    "        self._load_samples()\n",
    "        \n",
    "        if len(self.samples) > 0:\n",
    "            print(f\"üìä {split.capitalize()} dataset: {len(self.samples)} images\")\n",
    "            self._print_class_distribution()\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No images found in {self.data_dir}\")\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        \"\"\"Load all image paths with labels\"\"\"\n",
    "        for class_name in self.classes:\n",
    "            class_dir = self.data_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                extensions = ['*.jpeg', '*.jpg', '*.png', '*.bmp']\n",
    "                for ext in extensions:\n",
    "                    for img_path in class_dir.glob(ext):\n",
    "                        self.samples.append((str(img_path), self.class_to_idx[class_name]))\n",
    "                        self.class_counts[class_name] += 1\n",
    "    \n",
    "    def _print_class_distribution(self):\n",
    "        \"\"\"Print class distribution\"\"\"\n",
    "        total = len(self.samples)\n",
    "        print(f\"  Class distribution:\")\n",
    "        for class_name in self.classes:\n",
    "            count = self.class_counts[class_name]\n",
    "            percentage = (count / total) * 100 if total > 0 else 0\n",
    "            print(f\"    {class_name.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error loading {img_path}: {e}\")\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"Get all labels for stratified splitting\"\"\"\n",
    "        return [sample[1] for sample in self.samples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# üé® DATA TRANSFORMS\n",
    "# =============================================================================\n",
    "\n",
    "def get_kfold_transforms(augment_strength='light'):\n",
    "    \"\"\"Get transforms optimized for K-fold validation\"\"\"\n",
    "    \n",
    "    if augment_strength == 'light':\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    elif augment_strength == 'medium':\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((240, 240)),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.3),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:  # strong\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=20),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.15),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    # Validation transform (no augmentation)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã K-Fold Configuration:\n",
      "   K-Folds: 5\n",
      "   Stratified: True\n",
      "   Epochs per fold: 15\n",
      "   Batch size: 32\n",
      "   Learning rate: 0.001\n",
      "   Augmentation: light\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# ‚öôÔ∏è K-FOLD CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class KFoldConfig:\n",
    "    \"\"\"Enhanced configuration for K-Fold Cross Validation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # K-Fold settings\n",
    "        self.k_folds = 5\n",
    "        self.random_state = 42\n",
    "        self.use_stratified = True  # Maintain class distribution\n",
    "        \n",
    "        # Training settings\n",
    "        self.batch_size = 32  # Smaller batch for stability\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 15 # Enough for convergence\n",
    "        self.weight_decay = 1e-4\n",
    "        \n",
    "        # Early stopping\n",
    "        self.patience = 5  # Early stopping patience\n",
    "        self.min_delta = 0.001  # Minimum improvement\n",
    "        \n",
    "        # Data augmentation\n",
    "        self.augment_strength = 'light'  # Conservative for K-fold\n",
    "        \n",
    "        # Model settings\n",
    "        self.dropout_rate = 0.5\n",
    "        \n",
    "        # Other settings\n",
    "        self.save_all_models = True\n",
    "        self.detailed_logging = True\n",
    "        \n",
    "        print(f\"üìã K-Fold Configuration:\")\n",
    "        print(f\"   K-Folds: {self.k_folds}\")\n",
    "        print(f\"   Stratified: {self.use_stratified}\")\n",
    "        print(f\"   Epochs per fold: {self.num_epochs}\")\n",
    "        print(f\"   Batch size: {self.batch_size}\")\n",
    "        print(f\"   Learning rate: {self.learning_rate}\")\n",
    "        print(f\"   Augmentation: {self.augment_strength}\")\n",
    "\n",
    "# Initialize configuration\n",
    "kfold_config = KFoldConfig()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# üî• ENHANCED TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_single_fold_enhanced(model, train_loader, val_loader, fold_num, config, device=device):\n",
    "    \"\"\"Enhanced training for a single fold with better monitoring\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config.learning_rate, \n",
    "        weight_decay=config.weight_decay,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    fold_history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"üîÑ Training Fold {fold_num + 1}/{config.k_folds}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Fold {fold_num+1} Epoch {epoch+1} [Train]', leave=False)\n",
    "        for data, target in train_pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            train_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Fold {fold_num+1} Epoch {epoch+1} [Val]', leave=False)\n",
    "            for data, target in val_pbar:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                \n",
    "                val_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Calculate metrics\n",
    "        epoch_train_loss = train_loss / len(train_loader)\n",
    "        epoch_train_acc = 100.0 * train_correct / train_total\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_acc = 100.0 * val_correct / val_total\n",
    "        epoch_val_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(epoch_val_acc)\n",
    "        \n",
    "        # Save history\n",
    "        fold_history['train_loss'].append(epoch_train_loss)\n",
    "        fold_history['train_acc'].append(epoch_train_acc)\n",
    "        fold_history['val_loss'].append(epoch_val_loss)\n",
    "        fold_history['val_acc'].append(epoch_val_acc)\n",
    "        fold_history['val_f1'].append(epoch_val_f1)\n",
    "        fold_history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Print progress\n",
    "        if config.detailed_logging and ((epoch + 1) % 5 == 0 or epoch == config.num_epochs - 1):\n",
    "            print(f\"  Epoch {epoch+1:2d}: Train: {epoch_train_acc:6.2f}%, Val: {epoch_val_acc:6.2f}%, \"\n",
    "                  f\"F1: {epoch_val_f1:.4f}, LR: {optimizer.param_groups[0]['lr']:.2e}, Time: {epoch_time:.1f}s\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if epoch_val_acc > best_val_acc + config.min_delta:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config.patience:\n",
    "            print(f\"  Early stopping at epoch {epoch + 1} (no improvement for {config.patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return {\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'final_val_f1': fold_history['val_f1'][-1] if fold_history['val_f1'] else 0,\n",
    "        'history': fold_history,\n",
    "        'fold_num': fold_num,\n",
    "        'epochs_trained': len(fold_history['train_loss']),\n",
    "        'final_predictions': all_preds,\n",
    "        'final_targets': all_targets\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# üîÑ MAIN K-FOLD IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "def run_enhanced_kfold_validation(config):\n",
    "    \"\"\"Run enhanced K-Fold Cross Validation with comprehensive analysis\"\"\"\n",
    "    \n",
    "    print(\"üöÄ STARTING ENHANCED K-FOLD CROSS VALIDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load dataset\n",
    "    train_transform, val_transform = get_kfold_transforms(config.augment_strength)\n",
    "    full_dataset = LungCancerDataset(\n",
    "        data_dir=PROCESSED_DATA_PATH,\n",
    "        split='train',\n",
    "        transform=None,  # We'll apply transforms per fold\n",
    "        class_to_idx={'adenocarcinoma': 0, 'benign': 1}\n",
    "    )\n",
    "    \n",
    "    if len(full_dataset) == 0:\n",
    "        print(\"‚ùå No training data found!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Get labels for stratified splitting\n",
    "    all_labels = full_dataset.get_labels()\n",
    "    label_counts = Counter(all_labels)\n",
    "    print(f\"\\nüìä Dataset composition:\")\n",
    "    for label, count in label_counts.items():\n",
    "        class_name = full_dataset.classes[label]\n",
    "        print(f\"   {class_name.capitalize()}: {count} samples\")\n",
    "    \n",
    "    # Create K-Fold splitter\n",
    "    if config.use_stratified:\n",
    "        kfold = StratifiedKFold(\n",
    "            n_splits=config.k_folds, \n",
    "            shuffle=True, \n",
    "            random_state=config.random_state\n",
    "        )\n",
    "        split_generator = kfold.split(range(len(full_dataset)), all_labels)\n",
    "        print(f\"‚úÖ Using StratifiedKFold to maintain class distribution\")\n",
    "    else:\n",
    "        kfold = KFold(\n",
    "            n_splits=config.k_folds, \n",
    "            shuffle=True, \n",
    "            random_state=config.random_state\n",
    "        )\n",
    "        split_generator = kfold.split(range(len(full_dataset)))\n",
    "        print(f\"‚úÖ Using standard KFold\")\n",
    "    \n",
    "    # Results storage\n",
    "    fold_results = []\n",
    "    all_val_accs = []\n",
    "    all_val_f1s = []\n",
    "    all_fold_predictions = []\n",
    "    all_fold_targets = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run K-Fold validation\n",
    "    for fold_num, (train_indices, val_indices) in enumerate(split_generator):\n",
    "        \n",
    "        fold_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FOLD {fold_num + 1}/{config.k_folds}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Check class distribution in this fold\n",
    "        train_labels = [all_labels[i] for i in train_indices]\n",
    "        val_labels = [all_labels[i] for i in val_indices]\n",
    "        \n",
    "        train_label_counts = Counter(train_labels)\n",
    "        val_label_counts = Counter(val_labels)\n",
    "        \n",
    "        print(f\"üìä Fold {fold_num + 1} distribution:\")\n",
    "        print(f\"   Train: {dict(train_label_counts)} (total: {len(train_indices)})\")\n",
    "        print(f\"   Val:   {dict(val_label_counts)} (total: {len(val_indices)})\")\n",
    "        \n",
    "        # Create datasets for this fold\n",
    "        train_dataset = Subset(full_dataset, train_indices)\n",
    "        val_dataset = Subset(full_dataset, val_indices)\n",
    "        \n",
    "        # Apply transforms by temporarily modifying the dataset\n",
    "        original_transform = full_dataset.transform\n",
    "        \n",
    "        # Create data loaders with transforms\n",
    "        full_dataset.transform = train_transform\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "        full_dataset.transform = val_transform\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Create fresh model for this fold\n",
    "        model = LungCancerCNN(\n",
    "            num_classes=2, \n",
    "            dropout_rate=config.dropout_rate\n",
    "        ).to(device)\n",
    "        \n",
    "        print(f\"üì± Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        # Train the model\n",
    "        fold_result = train_single_fold_enhanced(\n",
    "            model, train_loader, val_loader, fold_num, config, device\n",
    "        )\n",
    "        \n",
    "        # Restore original transform\n",
    "        full_dataset.transform = original_transform\n",
    "        \n",
    "        # Save model if requested\n",
    "        if config.save_all_models:\n",
    "            model_path = MODELS_PATH / f\"cnn_fold_{fold_num + 1}_best.pth\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'fold_num': fold_num + 1,\n",
    "                'best_val_acc': fold_result['best_val_acc'],\n",
    "                'config': config.__dict__,\n",
    "                'train_indices': train_indices.tolist(),\n",
    "                'val_indices': val_indices.tolist()\n",
    "            }, model_path)\n",
    "            print(f\"üíæ Model saved: {model_path}\")\n",
    "        \n",
    "        fold_time = time.time() - fold_start_time\n",
    "        \n",
    "        # Store results\n",
    "        fold_result['fold_time_minutes'] = fold_time / 60\n",
    "        fold_results.append(fold_result)\n",
    "        all_val_accs.append(fold_result['best_val_acc'])\n",
    "        all_val_f1s.append(fold_result['final_val_f1'])\n",
    "        all_fold_predictions.extend(fold_result['final_predictions'])\n",
    "        all_fold_targets.extend(fold_result['final_targets'])\n",
    "        \n",
    "        print(f\"‚úÖ Fold {fold_num + 1} completed!\")\n",
    "        print(f\"   Best Val Acc: {fold_result['best_val_acc']:.2f}%\")\n",
    "        print(f\"   Final F1: {fold_result['final_val_f1']:.4f}\")\n",
    "        print(f\"   Time: {fold_time/60:.1f} minutes\")\n",
    "        print(f\"   Epochs: {fold_result['epochs_trained']}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate comprehensive statistics\n",
    "    results_summary = calculate_kfold_statistics(\n",
    "        all_val_accs, all_val_f1s, fold_results, total_time, config\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    save_kfold_results(results_summary, fold_results, all_fold_predictions, all_fold_targets)\n",
    "    \n",
    "    return results_summary, fold_results\n",
    "\n",
    "def calculate_kfold_statistics(all_val_accs, all_val_f1s, fold_results, total_time, config):\n",
    "    \"\"\"Calculate comprehensive K-fold statistics\"\"\"\n",
    "    \n",
    "    # Basic statistics\n",
    "    mean_acc = np.mean(all_val_accs)\n",
    "    std_acc = np.std(all_val_accs, ddof=1)  # Sample standard deviation\n",
    "    mean_f1 = np.mean(all_val_f1s)\n",
    "    std_f1 = np.std(all_val_f1s, ddof=1)\n",
    "    \n",
    "    # Confidence intervals (95%)\n",
    "    n = len(all_val_accs)\n",
    "    t_value = stats.t.ppf(0.975, n-1)  # 95% confidence interval\n",
    "    acc_ci_margin = t_value * std_acc / np.sqrt(n)\n",
    "    f1_ci_margin = t_value * std_f1 / np.sqrt(n)\n",
    "    \n",
    "    # Performance assessment\n",
    "    all_above_70 = all(acc >= 70 for acc in all_val_accs)\n",
    "    consistency_level = 'EXCELLENT' if std_acc < 2 else 'GOOD' if std_acc < 5 else 'MODERATE'\n",
    "    \n",
    "    # Average training time per fold\n",
    "    avg_fold_time = total_time / (60 * config.k_folds)\n",
    "    \n",
    "    results_summary = {\n",
    "        'k_folds': config.k_folds,\n",
    "        'accuracy_scores': all_val_accs,\n",
    "        'f1_scores': all_val_f1s,\n",
    "        'statistics': {\n",
    "            'mean_accuracy': float(mean_acc),\n",
    "            'std_accuracy': float(std_acc),\n",
    "            'mean_f1': float(mean_f1),\n",
    "            'std_f1': float(std_f1),\n",
    "            'min_accuracy': float(min(all_val_accs)),\n",
    "            'max_accuracy': float(max(all_val_accs)),\n",
    "            'min_f1': float(min(all_val_f1s)),\n",
    "            'max_f1': float(max(all_val_f1s)),\n",
    "            'accuracy_ci_lower': float(mean_acc - acc_ci_margin),\n",
    "            'accuracy_ci_upper': float(mean_acc + acc_ci_margin),\n",
    "            'f1_ci_lower': float(mean_f1 - f1_ci_margin),\n",
    "            'f1_ci_upper': float(mean_f1 + f1_ci_margin)\n",
    "        },\n",
    "        'performance_assessment': {\n",
    "            'all_folds_above_70': all_above_70,\n",
    "            'consistency_level': consistency_level,\n",
    "            'target_achieved': mean_acc >= 70,\n",
    "            'performance_grade': 'OUTSTANDING' if mean_acc > 95 else 'EXCELLENT' if mean_acc > 85 else 'GOOD' if mean_acc >= 70 else 'NEEDS_IMPROVEMENT'\n",
    "        },\n",
    "        'timing': {\n",
    "            'total_time_minutes': total_time / 60,\n",
    "            'average_fold_time_minutes': avg_fold_time,\n",
    "            'total_epochs': sum(fold['epochs_trained'] for fold in fold_results)\n",
    "        },\n",
    "        'config': {\n",
    "            'k_folds': config.k_folds,\n",
    "            'use_stratified': config.use_stratified,\n",
    "            'batch_size': config.batch_size,\n",
    "            'learning_rate': config.learning_rate,\n",
    "            'num_epochs': config.num_epochs,\n",
    "            'augment_strength': config.augment_strength\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "def save_kfold_results(results_summary, fold_results, all_predictions, all_targets):\n",
    "    \"\"\"Save K-fold results to files\"\"\"\n",
    "    \n",
    "    # Save main results\n",
    "    results_file = KFOLD_RESULTS_PATH / \"cnn_kfold_results.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        # Create a copy without non-serializable objects\n",
    "        save_data = {\n",
    "            'results_summary': results_summary,\n",
    "            'fold_details': []\n",
    "        }\n",
    "        \n",
    "        for fold_result in fold_results:\n",
    "            fold_data = {\n",
    "                'fold_num': fold_result['fold_num'],\n",
    "                'best_val_acc': fold_result['best_val_acc'],\n",
    "                'final_val_f1': fold_result['final_val_f1'],\n",
    "                'epochs_trained': fold_result['epochs_trained'],\n",
    "                'fold_time_minutes': fold_result['fold_time_minutes'],\n",
    "                'history': fold_result['history']\n",
    "            }\n",
    "            save_data['fold_details'].append(fold_data)\n",
    "        \n",
    "        json.dump(save_data, f, indent=2)\n",
    "    \n",
    "    # Save predictions for further analysis\n",
    "    predictions_file = KFOLD_RESULTS_PATH / \"cnn_kfold_predictions.json\"\n",
    "    with open(predictions_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'predictions': [int(p) for p in all_predictions],\n",
    "            'targets': [int(t) for t in all_targets],\n",
    "            'class_names': ['adenocarcinoma', 'benign']\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Results saved:\")\n",
    "    print(f\"   Main results: {results_file}\")\n",
    "    print(f\"   Predictions: {predictions_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# üìä ENHANCED VISUALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_comprehensive_kfold_visualizations(results_summary, fold_results):\n",
    "    \"\"\"Create comprehensive K-fold visualizations\"\"\"\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Main title\n",
    "    fig.suptitle('CNN K-Fold Cross-Validation Comprehensive Analysis', \n",
    "                fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # 1. Accuracy per fold (top-left)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    bars = ax1.bar(range(1, kfold_config.k_folds + 1), results_summary['accuracy_scores'], \n",
    "                   color='skyblue', alpha=0.8, edgecolor='navy', linewidth=2)\n",
    "    ax1.axhline(y=results_summary['statistics']['mean_accuracy'], color='red', \n",
    "                linestyle='--', linewidth=2, label=f\"Mean: {results_summary['statistics']['mean_accuracy']:.2f}%\")\n",
    "    ax1.axhline(y=70, color='green', linestyle='--', alpha=0.7, linewidth=2, label='70% Target')\n",
    "    \n",
    "    # Add confidence interval\n",
    "    mean_acc = results_summary['statistics']['mean_accuracy']\n",
    "    ci_lower = results_summary['statistics']['accuracy_ci_lower']\n",
    "    ci_upper = results_summary['statistics']['accuracy_ci_upper']\n",
    "    ax1.fill_between([0.5, kfold_config.k_folds + 0.5], ci_lower, ci_upper, \n",
    "                     alpha=0.2, color='red', label=f'95% CI')\n",
    "    \n",
    "    ax1.set_title('Validation Accuracy per Fold', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Fold Number')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_ylim(60, 100)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, acc) in enumerate(zip(bars, results_summary['accuracy_scores'])):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. F1 Score per fold (top-right)\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    bars2 = ax2.bar(range(1, kfold_config.k_folds + 1), results_summary['f1_scores'], \n",
    "                    color='lightgreen', alpha=0.8, edgecolor='darkgreen', linewidth=2)\n",
    "    ax2.axhline(y=results_summary['statistics']['mean_f1'], color='red', \n",
    "                linestyle='--', linewidth=2, label=f\"Mean: {results_summary['statistics']['mean_f1']:.4f}\")\n",
    "    ax2.set_title('F1 Score per Fold', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Fold Number')\n",
    "    ax2.set_ylabel('F1 Score')\n",
    "    ax2.set_ylim(0.85, 1.0)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, f1) in enumerate(zip(bars2, results_summary['f1_scores'])):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002, \n",
    "                f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Training curves for all folds (middle-left, spans 2 columns)\n",
    "    ax3 = fig.add_subplot(gs[1, :2])\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, kfold_config.k_folds))\n",
    "    \n",
    "    for i, fold_result in enumerate(fold_results):\n",
    "        history = fold_result['history']\n",
    "        epochs = range(1, len(history['val_acc']) + 1)\n",
    "        ax3.plot(epochs, history['val_acc'], label=f'Fold {i+1}', \n",
    "                color=colors[i], alpha=0.8, linewidth=2)\n",
    "    \n",
    "    ax3.axhline(y=70, color='green', linestyle='--', alpha=0.7, linewidth=2, label='70% Target')\n",
    "    ax3.axhline(y=results_summary['statistics']['mean_accuracy'], color='red', \n",
    "                linestyle='--', linewidth=2, label=f\"Mean Final: {results_summary['statistics']['mean_accuracy']:.1f}%\")\n",
    "    ax3.set_title('Validation Accuracy Curves - All Folds', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Box plot and violin plot (middle-right)\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    \n",
    "    # Box plot\n",
    "    bp = ax4.boxplot([results_summary['accuracy_scores']], labels=['Accuracy'], \n",
    "                     patch_artist=True, showmeans=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    bp['boxes'][0].set_alpha(0.7)\n",
    "    \n",
    "    # Scatter individual points\n",
    "    y_data = results_summary['accuracy_scores']\n",
    "    x_data = [1] * len(y_data)\n",
    "    ax4.scatter(x_data, y_data, alpha=0.8, color='red', s=50, zorder=10)\n",
    "    \n",
    "    ax4.axhline(y=70, color='green', linestyle='--', alpha=0.7, linewidth=2, label='70% Target')\n",
    "    ax4.set_title('Accuracy Distribution', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Accuracy (%)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    # 5. Statistical summary (middle-right-2)\n",
    "    ax5 = fig.add_subplot(gs[1, 3])\n",
    "    ax5.axis('off')\n",
    "    \n",
    "    stats_text = f\"\"\"Statistical Summary:\n",
    "    \n",
    "Accuracy:\n",
    "  Mean: {results_summary['statistics']['mean_accuracy']:.2f}%\n",
    "  Std: {results_summary['statistics']['std_accuracy']:.2f}%\n",
    "  95% CI: [{results_summary['statistics']['accuracy_ci_lower']:.2f}%, \n",
    "           {results_summary['statistics']['accuracy_ci_upper']:.2f}%]\n",
    "  Range: [{results_summary['statistics']['min_accuracy']:.2f}%, \n",
    "          {results_summary['statistics']['max_accuracy']:.2f}%]\n",
    "\n",
    "F1 Score:\n",
    "  Mean: {results_summary['statistics']['mean_f1']:.4f}\n",
    "  Std: {results_summary['statistics']['std_f1']:.4f}\n",
    "  Range: [{results_summary['statistics']['min_f1']:.4f}, \n",
    "          {results_summary['statistics']['max_f1']:.4f}]\n",
    "\n",
    "Performance:\n",
    "  Target Achieved: {'‚úÖ' if results_summary['performance_assessment']['target_achieved'] else '‚ùå'}\n",
    "  All Folds >70%: {'‚úÖ' if results_summary['performance_assessment']['all_folds_above_70'] else '‚ùå'}\n",
    "  Consistency: {results_summary['performance_assessment']['consistency_level']}\n",
    "  Grade: {results_summary['performance_assessment']['performance_grade']}\n",
    "\"\"\"\n",
    "    \n",
    "    ax5.text(0.05, 0.95, stats_text, transform=ax5.transAxes, fontsize=10, \n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    # 6. Training time analysis (bottom-left)\n",
    "    ax6 = fig.add_subplot(gs[2, 0])\n",
    "    fold_times = [fold['fold_time_minutes'] for fold in fold_results]\n",
    "    bars6 = ax6.bar(range(1, kfold_config.k_folds + 1), fold_times, \n",
    "                    color='orange', alpha=0.8, edgecolor='darkorange')\n",
    "    ax6.axhline(y=np.mean(fold_times), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(fold_times):.1f} min')\n",
    "    ax6.set_title('Training Time per Fold', fontsize=14, fontweight='bold')\n",
    "    ax6.set_xlabel('Fold Number')\n",
    "    ax6.set_ylabel('Time (minutes)')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Epochs trained per fold (bottom-middle-left)\n",
    "    ax7 = fig.add_subplot(gs[2, 1])\n",
    "    epochs_trained = [fold['epochs_trained'] for fold in fold_results]\n",
    "    bars7 = ax7.bar(range(1, kfold_config.k_folds + 1), epochs_trained, \n",
    "                    color='purple', alpha=0.8, edgecolor='darkpurple')\n",
    "    ax7.axhline(y=np.mean(epochs_trained), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(epochs_trained):.1f}')\n",
    "    ax7.set_title('Epochs Trained per Fold', fontsize=14, fontweight='bold')\n",
    "    ax7.set_xlabel('Fold Number')\n",
    "    ax7.set_ylabel('Epochs')\n",
    "    ax7.legend()\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. Learning curves comparison (bottom-middle-right)\n",
    "    ax8 = fig.add_subplot(gs[2, 2])\n",
    "    for i, fold_result in enumerate(fold_results):\n",
    "        history = fold_result['history']\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        ax8.plot(epochs, history['train_loss'], '--', alpha=0.7, color=colors[i])\n",
    "        ax8.plot(epochs, history['val_loss'], '-', alpha=0.8, color=colors[i], \n",
    "                label=f'Fold {i+1}')\n",
    "    \n",
    "    ax8.set_title('Loss Curves - All Folds', fontsize=14, fontweight='bold')\n",
    "    ax8.set_xlabel('Epoch')\n",
    "    ax8.set_ylabel('Loss')\n",
    "    ax8.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Model configuration (bottom-right)\n",
    "    ax9 = fig.add_subplot(gs[2, 3])\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    config_text = f\"\"\"Model Configuration:\n",
    "    \n",
    "K-Fold Settings:\n",
    "  Folds: {results_summary['config']['k_folds']}\n",
    "  Stratified: {results_summary['config']['use_stratified']}\n",
    "  \n",
    "Training Settings:\n",
    "  Batch Size: {results_summary['config']['batch_size']}\n",
    "  Learning Rate: {results_summary['config']['learning_rate']}\n",
    "  Max Epochs: {results_summary['config']['num_epochs']}\n",
    "  Augmentation: {results_summary['config']['augment_strength']}\n",
    "  \n",
    "Timing:\n",
    "  Total Time: {results_summary['timing']['total_time_minutes']:.1f} min\n",
    "  Avg per Fold: {results_summary['timing']['average_fold_time_minutes']:.1f} min\n",
    "  Total Epochs: {results_summary['timing']['total_epochs']}\n",
    "\"\"\"\n",
    "    \n",
    "    ax9.text(0.05, 0.95, config_text, transform=ax9.transAxes, fontsize=10, \n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    # 10. Confusion matrix for all folds combined (bottom span)\n",
    "    if len(fold_results) > 0:\n",
    "        ax10 = fig.add_subplot(gs[3, :2])\n",
    "        \n",
    "        # Combine all predictions\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        for fold in fold_results:\n",
    "            all_preds.extend(fold['final_predictions'])\n",
    "            all_targets.extend(fold['final_targets'])\n",
    "        \n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Adenocarcinoma', 'Benign'], \n",
    "                   yticklabels=['Adenocarcinoma', 'Benign'],\n",
    "                   ax=ax10, square=True)\n",
    "        ax10.set_title('Combined Confusion Matrix (All Folds)', fontsize=14, fontweight='bold')\n",
    "        ax10.set_xlabel('Predicted')\n",
    "        ax10.set_ylabel('Actual')\n",
    "        \n",
    "        # Calculate and display metrics\n",
    "        overall_acc = np.trace(cm) / np.sum(cm)\n",
    "        ax10.text(0.02, 0.98, f'Overall Accuracy: {overall_acc:.3f}', \n",
    "                 transform=ax10.transAxes, fontsize=12, fontweight='bold',\n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 11. Performance comparison (bottom-right span)\n",
    "    ax11 = fig.add_subplot(gs[3, 2:])\n",
    "    \n",
    "    # Create performance radar chart or summary\n",
    "    metrics = ['Mean Accuracy', 'Consistency', 'Speed', 'Stability']\n",
    "    values = [\n",
    "        results_summary['statistics']['mean_accuracy'] / 100,  # Normalize to 0-1\n",
    "        max(0, (5 - results_summary['statistics']['std_accuracy']) / 5),  # Lower std = better\n",
    "        max(0, (30 - results_summary['timing']['average_fold_time_minutes']) / 30),  # Faster = better\n",
    "        len([acc for acc in results_summary['accuracy_scores'] if acc >= 70]) / kfold_config.k_folds  # Fraction above 70%\n",
    "    ]\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "    values += values[:1]  # Complete the circle\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax11 = plt.subplot(gs[3, 2:], projection='polar')\n",
    "    ax11.plot(angles, values, 'o-', linewidth=2, color='blue', alpha=0.8)\n",
    "    ax11.fill(angles, values, alpha=0.25, color='blue')\n",
    "    ax11.set_xticks(angles[:-1])\n",
    "    ax11.set_xticklabels(metrics)\n",
    "    ax11.set_ylim(0, 1)\n",
    "    ax11.set_title('Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax11.grid(True)\n",
    "    \n",
    "    # Save the comprehensive visualization\n",
    "    plt.savefig(KFOLD_RESULTS_PATH / 'cnn_kfold_comprehensive_analysis.png', \n",
    "                dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "def print_detailed_kfold_analysis(results_summary, fold_results):\n",
    "    \"\"\"Print detailed statistical analysis of K-fold results\"\"\"\n",
    "    \n",
    "    print(f\"\\nüî¨ DETAILED K-FOLD STATISTICAL ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Overall results\n",
    "    print(f\"üìä OVERALL RESULTS:\")\n",
    "    print(f\"   Mean Accuracy: {results_summary['statistics']['mean_accuracy']:.2f}% ¬± {results_summary['statistics']['std_accuracy']:.2f}%\")\n",
    "    print(f\"   Mean F1 Score: {results_summary['statistics']['mean_f1']:.4f} ¬± {results_summary['statistics']['std_f1']:.4f}\")\n",
    "    print(f\"   95% Confidence Interval: [{results_summary['statistics']['accuracy_ci_lower']:.2f}%, {results_summary['statistics']['accuracy_ci_upper']:.2f}%]\")\n",
    "    \n",
    "    # Individual fold results\n",
    "    print(f\"\\nüìã INDIVIDUAL FOLD RESULTS:\")\n",
    "    print(f\"{'Fold':<6} {'Accuracy':<10} {'F1 Score':<10} {'Epochs':<8} {'Time (min)':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (acc, f1, fold) in enumerate(zip(results_summary['accuracy_scores'], \n",
    "                                           results_summary['f1_scores'], \n",
    "                                           fold_results)):\n",
    "        print(f\"{i+1:<6} {acc:<10.2f} {f1:<10.4f} {fold['epochs_trained']:<8} {fold['fold_time_minutes']:<12.1f}\")\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(f\"\\nüìà STATISTICAL ANALYSIS:\")\n",
    "    print(f\"   Range: {results_summary['statistics']['min_accuracy']:.2f}% - {results_summary['statistics']['max_accuracy']:.2f}%\")\n",
    "    print(f\"   Coefficient of Variation: {(results_summary['statistics']['std_accuracy']/results_summary['statistics']['mean_accuracy']*100):.2f}%\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    print(f\"\\nüéØ PERFORMANCE ASSESSMENT:\")\n",
    "    assessment = results_summary['performance_assessment']\n",
    "    print(f\"   Target Achievement (>70%): {'‚úÖ SUCCESS' if assessment['target_achieved'] else '‚ùå FAILED'}\")\n",
    "    print(f\"   All Folds Above 70%: {'‚úÖ YES' if assessment['all_folds_above_70'] else '‚ùå NO'}\")\n",
    "    print(f\"   Consistency Level: {assessment['consistency_level']}\")\n",
    "    print(f\"   Performance Grade: {assessment['performance_grade']}\")\n",
    "    \n",
    "    # Training efficiency\n",
    "    print(f\"\\n‚è±Ô∏è  TRAINING EFFICIENCY:\")\n",
    "    timing = results_summary['timing']\n",
    "    print(f\"   Total Training Time: {timing['total_time_minutes']:.1f} minutes\")\n",
    "    print(f\"   Average per Fold: {timing['average_fold_time_minutes']:.1f} minutes\")\n",
    "    print(f\"   Total Epochs Trained: {timing['total_epochs']}\")\n",
    "    print(f\"   Average Epochs per Fold: {timing['total_epochs']/kfold_config.k_folds:.1f}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    if assessment['target_achieved']:\n",
    "        if assessment['consistency_level'] == 'EXCELLENT':\n",
    "            print(\"   üèÜ Outstanding performance! Model is ready for production.\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ Good performance, but consider tuning for better consistency.\")\n",
    "    else:\n",
    "        print(\"   üîß Performance below target. Consider:\")\n",
    "        print(\"      ‚Ä¢ Increasing model capacity\")\n",
    "        print(\"      ‚Ä¢ Adjusting hyperparameters\")\n",
    "        print(\"      ‚Ä¢ Adding more data augmentation\")\n",
    "        print(\"      ‚Ä¢ Collecting more training data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ENHANCED CNN K-FOLD CROSS-VALIDATION\n",
      "============================================================\n",
      "üöÄ STARTING ENHANCED K-FOLD CROSS VALIDATION\n",
      "============================================================\n",
      "üìä Train dataset: 16000 images\n",
      "  Class distribution:\n",
      "    Adenocarcinoma: 8000 (50.0%)\n",
      "    Benign: 8000 (50.0%)\n",
      "\n",
      "üìä Dataset composition:\n",
      "   Adenocarcinoma: 8000 samples\n",
      "   Benign: 8000 samples\n",
      "‚úÖ Using StratifiedKFold to maintain class distribution\n",
      "\n",
      "==================================================\n",
      "FOLD 1/5\n",
      "==================================================\n",
      "üìä Fold 1 distribution:\n",
      "   Train: {0: 6400, 1: 6400} (total: 12800)\n",
      "   Val:   {0: 1600, 1: 1600} (total: 3200)\n",
      "üì± Model parameters: 5,929,570\n",
      "üîÑ Training Fold 1/5\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch  5: Train:  98.52%, Val:  99.47%, F1: 0.9947, LR: 1.00e-03, Time: 144.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10: Train:  99.14%, Val:  99.78%, F1: 0.9978, LR: 1.00e-03, Time: 158.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Early stopping at epoch 13 (no improvement for 5 epochs)\n",
      "üíæ Model saved: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\models\\cnn_fold_1_best.pth\n",
      "‚úÖ Fold 1 completed!\n",
      "   Best Val Acc: 99.97%\n",
      "   Final F1: 0.9994\n",
      "   Time: 31.5 minutes\n",
      "   Epochs: 13\n",
      "\n",
      "==================================================\n",
      "FOLD 2/5\n",
      "==================================================\n",
      "üìä Fold 2 distribution:\n",
      "   Train: {0: 6400, 1: 6400} (total: 12800)\n",
      "   Val:   {0: 1600, 1: 1600} (total: 3200)\n",
      "üì± Model parameters: 5,929,570\n",
      "üîÑ Training Fold 2/5\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch  5: Train:  98.40%, Val:  99.47%, F1: 0.9947, LR: 1.00e-03, Time: 145.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10: Train:  98.87%, Val:  99.59%, F1: 0.9959, LR: 5.00e-04, Time: 148.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Early stopping at epoch 11 (no improvement for 5 epochs)\n",
      "üíæ Model saved: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\models\\cnn_fold_2_best.pth\n",
      "‚úÖ Fold 2 completed!\n",
      "   Best Val Acc: 99.91%\n",
      "   Final F1: 0.9991\n",
      "   Time: 27.9 minutes\n",
      "   Epochs: 11\n",
      "\n",
      "==================================================\n",
      "FOLD 3/5\n",
      "==================================================\n",
      "üìä Fold 3 distribution:\n",
      "   Train: {0: 6400, 1: 6400} (total: 12800)\n",
      "   Val:   {0: 1600, 1: 1600} (total: 3200)\n",
      "üì± Model parameters: 5,929,570\n",
      "üîÑ Training Fold 3/5\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch  5: Train:  98.31%, Val:  99.56%, F1: 0.9956, LR: 1.00e-03, Time: 161.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10: Train:  99.16%, Val:  99.78%, F1: 0.9978, LR: 1.00e-03, Time: 141.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15: Train:  99.60%, Val:  99.97%, F1: 0.9997, LR: 5.00e-04, Time: 142.2s\n",
      "üíæ Model saved: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\models\\cnn_fold_3_best.pth\n",
      "‚úÖ Fold 3 completed!\n",
      "   Best Val Acc: 99.97%\n",
      "   Final F1: 0.9997\n",
      "   Time: 36.6 minutes\n",
      "   Epochs: 15\n",
      "\n",
      "==================================================\n",
      "FOLD 4/5\n",
      "==================================================\n",
      "üìä Fold 4 distribution:\n",
      "   Train: {0: 6400, 1: 6400} (total: 12800)\n",
      "   Val:   {0: 1600, 1: 1600} (total: 3200)\n",
      "üì± Model parameters: 5,929,570\n",
      "üîÑ Training Fold 4/5\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch  5: Train:  98.64%, Val:  99.81%, F1: 0.9981, LR: 1.00e-03, Time: 157.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Early stopping at epoch 8 (no improvement for 5 epochs)\n",
      "üíæ Model saved: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\models\\cnn_fold_4_best.pth\n",
      "‚úÖ Fold 4 completed!\n",
      "   Best Val Acc: 99.81%\n",
      "   Final F1: 0.9900\n",
      "   Time: 23.9 minutes\n",
      "   Epochs: 8\n",
      "\n",
      "==================================================\n",
      "FOLD 5/5\n",
      "==================================================\n",
      "üìä Fold 5 distribution:\n",
      "   Train: {0: 6400, 1: 6400} (total: 12800)\n",
      "   Val:   {0: 1600, 1: 1600} (total: 3200)\n",
      "üì± Model parameters: 5,929,570\n",
      "üîÑ Training Fold 5/5\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch  5: Train:  98.48%, Val:  99.69%, F1: 0.9969, LR: 1.00e-03, Time: 173.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10: Train:  99.12%, Val:  99.41%, F1: 0.9941, LR: 1.00e-03, Time: 161.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# üéØ MAIN EXECUTION: ENHANCED K-FOLD VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ ENHANCED CNN K-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run K-fold validation\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    results_summary, fold_results = run_enhanced_kfold_validation(kfold_config)\n",
    "    \n",
    "    if results_summary is not None:\n",
    "        # Create comprehensive visualizations\n",
    "        print(f\"\\nüìä Creating comprehensive visualizations...\")\n",
    "        create_comprehensive_kfold_visualizations(results_summary, fold_results)\n",
    "        \n",
    "        # Print detailed analysis\n",
    "        print_detailed_kfold_analysis(results_summary, fold_results)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"\\nüéâ K-FOLD VALIDATION COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üìä FINAL RESULTS SUMMARY:\")\n",
    "        print(f\"   Mean Accuracy: {results_summary['statistics']['mean_accuracy']:.2f}% ¬± {results_summary['statistics']['std_accuracy']:.2f}%\")\n",
    "        print(f\"   Target Achieved: {'‚úÖ YES' if results_summary['performance_assessment']['target_achieved'] else '‚ùå NO'}\")\n",
    "        print(f\"   Consistency: {results_summary['performance_assessment']['consistency_level']}\")\n",
    "        print(f\"   Total Time: {total_time/60:.1f} minutes\")\n",
    "        \n",
    "        # Update main configuration\n",
    "        config['cnn_kfold'] = {\n",
    "            'status': 'completed',\n",
    "            'mean_accuracy': float(results_summary['statistics']['mean_accuracy']),\n",
    "            'std_accuracy': float(results_summary['statistics']['std_accuracy']),\n",
    "            'mean_f1': float(results_summary['statistics']['mean_f1']),\n",
    "            'target_achieved': results_summary['performance_assessment']['target_achieved'],\n",
    "            'all_folds_above_70': results_summary['performance_assessment']['all_folds_above_70'],\n",
    "            'consistency_level': results_summary['performance_assessment']['consistency_level'],\n",
    "            'performance_grade': results_summary['performance_assessment']['performance_grade'],\n",
    "            'k_folds': kfold_config.k_folds,\n",
    "            'ready_for_scatnet': True\n",
    "        }\n",
    "        \n",
    "        # Save updated config\n",
    "        config_path = PROJECT_ROOT / \"config.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüíæ Files Generated:\")\n",
    "        print(f\"   üìä Results: {KFOLD_RESULTS_PATH}/cnn_kfold_results.json\")\n",
    "        print(f\"   üîÆ Predictions: {KFOLD_RESULTS_PATH}/cnn_kfold_predictions.json\")\n",
    "        print(f\"   üìà Visualization: {KFOLD_RESULTS_PATH}/cnn_kfold_comprehensive_analysis.png\")\n",
    "        print(f\"   üèÜ Models: {MODELS_PATH}/cnn_fold_*_best.pth\")\n",
    "        \n",
    "        print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "        print(f\"1. üìù Proceed to: 03_scatnet_implementation/01_scatnet_training.ipynb\")\n",
    "        print(f\"2. üî¨ Implement ScatNet with same shared classifier\")\n",
    "        print(f\"3. üìä Compare CNN vs ScatNet performance\")\n",
    "        print(f\"4. üéØ Ensure ScatNet also achieves >70% accuracy\")\n",
    "        print(f\"5. üìã Document results for explainability analysis\")\n",
    "        \n",
    "        print(f\"\\nüìö DELIVERABLES COMPLETED:\")\n",
    "        print(f\"‚úÖ 5-fold cross-validation implemented\")\n",
    "        print(f\"‚úÖ Mean accuracy and F1 scores calculated\")\n",
    "        print(f\"‚úÖ Statistical significance established\")\n",
    "        print(f\"‚úÖ Model consistency validated\")\n",
    "        print(f\"‚úÖ Ready for ScatNet comparison\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå K-fold validation failed!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during K-fold validation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\nüéØ CNN K-FOLD VALIDATION COMPLETE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Main config updated with K-fold results!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Manually update config.json with provided K-fold results ---\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Fill in the actual results from your K-fold output\n",
    "mean_acc = 99.92  # average of [99.97, 99.91, 99.97, 99.81, 99.94]\n",
    "std_acc = 0.064   # calculated sample std\n",
    "mean_f1 = 0.9975  # average of [0.9994, 0.9991, 0.9997, 0.9900, 0.9972]\n",
    "target_achieved = True\n",
    "all_folds_above_70 = True\n",
    "consistency_level = \"EXCELLENT\"\n",
    "performance_grade = \"OUTSTANDING\"\n",
    "k_folds = 5\n",
    "\n",
    "config_path = Path(\"config.json\")\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "else:\n",
    "    config = {}\n",
    "\n",
    "config['cnn_kfold'] = {\n",
    "    'status': 'completed',\n",
    "    'mean_accuracy': mean_acc,\n",
    "    'std_accuracy': std_acc,\n",
    "    'mean_f1': mean_f1,\n",
    "    'target_achieved': target_achieved,\n",
    "    'all_folds_above_70': all_folds_above_70,\n",
    "    'consistency_level': consistency_level,\n",
    "    'performance_grade': performance_grade,\n",
    "    'k_folds': k_folds,\n",
    "    'ready_for_scatnet': True\n",
    "}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Main config updated with K-fold results!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
