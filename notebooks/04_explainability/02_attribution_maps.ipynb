{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Attribution Maps Generation - CNN vs ScatNet\n",
      "============================================================\n",
      "üìã This notebook generates attribution maps for CNN and ScatNet models\n",
      "üß† Uses DeepLIFT for CNN and perturbation analysis for ScatNet\n",
      "\n",
      "============================================================\n",
      "üìã ATTRIBUTION MAPS GENERATION: READY\n",
      "üöÄ Starting attribution generation...\n",
      "\n",
      "üöÄ MAIN ATTRIBUTION MAP GENERATION\n",
      "============================================================\n",
      "üìÅ Project paths:\n",
      "   Root: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\n",
      "   Models: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\models\n",
      "   Results: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\results\n",
      "   Explainability: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\results\\explainability\n",
      "\n",
      "üîç Path verification:\n",
      "   Project root exists: True\n",
      "   Models folder exists: True\n",
      "   Results folder exists: True\n",
      "\n",
      "üîç Looking for models...\n",
      "üìã Available .pth files: ['best_cnn_model.pth', 'best_scatnet_model.pth', 'cnn_architecture.pth', 'cnn_final_trained.pth', 'cnn_fold_1_best.pth', 'cnn_fold_2_best.pth', 'cnn_fold_3_best.pth', 'cnn_fold_4_best.pth', 'cnn_fold_5_best.pth', 'scatnet_final_trained.pth']\n",
      "‚úÖ CNN model found: best_cnn_model.pth\n",
      "‚úÖ ScatNet model found: best_scatnet_model.pth\n",
      "\n",
      "üìÅ Searching for data...\n",
      "‚úÖ Data found at: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\data\\raw\n",
      "üîß Using device: cuda\n",
      "\n",
      "üì• Creating demo models...\n",
      "‚úÖ CNN demo model ready\n",
      "‚úÖ ScatNet demo model ready\n",
      "\n",
      "üìÅ Loading dataset...\n",
      "üìÇ Loading dataset from: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\data\\raw\n",
      "   Adenocarcinoma: 10000 images\n",
      "   Benign: 10000 images\n",
      "üìã Dataset loaded: 5 images\n",
      "   Adenocarcinoma: 3\n",
      "   Benign: 2\n",
      "üß† DeepLIFT initialized for SimpleCNN\n",
      "üß† Attribution Map Generator initialized\n",
      "   Device: cuda\n",
      "   CNN explainer: ‚úÖ DeepLIFT\n",
      "   ScatNet handler: ‚úÖ Perturbation Analysis\n",
      "\n",
      "üéØ Generating attribution maps...\n",
      "\n",
      "--- Sample 1/3 ---\n",
      "\n",
      "üîç Generating attributions for: 0245.jpg\n",
      "   CNN prediction: Class 0 (confidence: 0.505)\n",
      "   üîç CNN attribution with zero reference...\n",
      "   üîç CNN attribution with mean reference...\n",
      "   üîç CNN attribution with gaussian_blur reference...\n",
      "   üîç CNN attribution with random_noise reference...\n",
      "   ScatNet prediction: Class 0 (confidence: 0.586)\n",
      "üîç Computing ScatNet attribution via perturbation analysis...\n",
      "‚úÖ ScatNet perturbation attribution complete\n",
      "   üíæ Saved 5 attribution maps\n",
      "\n",
      "--- Sample 2/3 ---\n",
      "\n",
      "üîç Generating attributions for: 2416.jpg\n",
      "   CNN prediction: Class 1 (confidence: 0.558)\n",
      "   üîç CNN attribution with zero reference...\n",
      "   üîç CNN attribution with mean reference...\n",
      "   üîç CNN attribution with gaussian_blur reference...\n",
      "   üîç CNN attribution with random_noise reference...\n",
      "   ScatNet prediction: Class 0 (confidence: 0.559)\n",
      "üîç Computing ScatNet attribution via perturbation analysis...\n",
      "‚úÖ ScatNet perturbation attribution complete\n",
      "   üíæ Saved 5 attribution maps\n",
      "\n",
      "--- Sample 3/3 ---\n",
      "\n",
      "üîç Generating attributions for: 3278.jpg\n",
      "   CNN prediction: Class 1 (confidence: 0.530)\n",
      "   üîç CNN attribution with zero reference...\n",
      "   üîç CNN attribution with mean reference...\n",
      "   üîç CNN attribution with gaussian_blur reference...\n",
      "   üîç CNN attribution with random_noise reference...\n",
      "   ScatNet prediction: Class 0 (confidence: 0.577)\n",
      "üîç Computing ScatNet attribution via perturbation analysis...\n",
      "‚úÖ ScatNet perturbation attribution complete\n",
      "   üíæ Saved 5 attribution maps\n",
      "\n",
      "üíæ Results summary saved: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\results\\explainability\\attribution_results_summary.json\n",
      "\n",
      "üìÇ Verifying saved files:\n",
      "   Attribution maps: 15 files\n",
      "     - 0245_cnn_gaussian_blur.png\n",
      "     - 0245_cnn_mean.png\n",
      "     - 0245_cnn_random_noise.png\n",
      "     - 0245_cnn_zero.png\n",
      "     - 0245_scatnet_perturbation.png\n",
      "\n",
      "üéâ ATTRIBUTION MAP GENERATION COMPLETE!\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Samples processed: 3\n",
      "   ‚Ä¢ CNN attributions: ‚úÖ\n",
      "   ‚Ä¢ ScatNet attributions: ‚úÖ\n",
      "   ‚Ä¢ Files saved to: D:\\University\\4th Semester\\4. Visual Intelligence\\Project\\results\\explainability\n",
      "\n",
      "üéâ Attribution generation completed successfully!\n",
      "\n",
      "============================================================\n",
      "üìã SCRIPT COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# üìã ATTRIBUTION MAPS GENERATION - CNN vs ScatNet\n",
    "# Visual Intelligence Project - Phase 3: Explainability\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üéØ Attribution Maps Generation - CNN vs ScatNet\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìã This notebook generates attribution maps for CNN and ScatNet models\")\n",
    "print(\"üß† Uses DeepLIFT for CNN and perturbation analysis for ScatNet\")\n",
    "\n",
    "# =============================================================================\n",
    "# üß† DEEPLIFT IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "class DeepLIFTFromScratch:\n",
    "    \"\"\"Simplified DeepLIFT implementation for attribution analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, model, reference_input=None):\n",
    "        self.model = model\n",
    "        self.reference_input = reference_input\n",
    "        print(f\"üß† DeepLIFT initialized for {model.__class__.__name__}\")\n",
    "    \n",
    "    def set_reference(self, reference_input):\n",
    "        self.reference_input = reference_input\n",
    "    \n",
    "    def compute_attributions(self, input_tensor, target_class=None):\n",
    "        \"\"\"Simplified attribution using gradient * input\"\"\"\n",
    "        self.model.eval()\n",
    "        input_tensor.requires_grad_(True)\n",
    "        \n",
    "        output = self.model(input_tensor)\n",
    "        if target_class is None:\n",
    "            target_class = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        # Simple gradient * input attribution\n",
    "        target_output = output[0, target_class]\n",
    "        target_output.backward()\n",
    "        \n",
    "        attribution = input_tensor.grad * input_tensor\n",
    "        input_tensor.requires_grad_(False)\n",
    "        \n",
    "        return attribution.detach()\n",
    "\n",
    "class DeepLIFTVisualizer:\n",
    "    \"\"\"Visualization utilities for attribution maps\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_attribution_map(input_image, attributions, title=\"Attribution Map\", save_path=None):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Original image\n",
    "        if input_image.dim() == 4:\n",
    "            img = input_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        else:\n",
    "            img = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        \n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Attribution\n",
    "        attr = torch.sum(torch.abs(attributions.squeeze(0)), dim=0).cpu().numpy()\n",
    "        im = axes[1].imshow(attr, cmap='hot')\n",
    "        axes[1].set_title('Attribution')\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[1])\n",
    "        \n",
    "        # Overlay\n",
    "        axes[2].imshow(img, alpha=0.7)\n",
    "        axes[2].imshow(attr, cmap='hot', alpha=0.5)\n",
    "        axes[2].set_title('Overlay')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# =============================================================================\n",
    "# üì∏ DATASET LOADER\n",
    "# =============================================================================\n",
    "\n",
    "class LungCancerDataset(Dataset):\n",
    "    \"\"\"Dataset loader for lung cancer images\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, transform=None, max_samples=None):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        print(f\"üìÇ Loading dataset from: {self.data_path}\")\n",
    "        \n",
    "        # Load adenocarcinoma images (class 0)\n",
    "        adeno_path = self.data_path / \"adenocarcinoma\"\n",
    "        if adeno_path.exists():\n",
    "            adeno_files = []\n",
    "            for ext in [\"*.jpeg\", \"*.jpg\", \"*.png\", \"*.JPEG\", \"*.JPG\", \"*.PNG\"]:\n",
    "                adeno_files.extend(list(adeno_path.glob(ext)))\n",
    "            \n",
    "            self.image_paths.extend(adeno_files)\n",
    "            self.labels.extend([0] * len(adeno_files))\n",
    "            print(f\"   Adenocarcinoma: {len(adeno_files)} images\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Adenocarcinoma directory not found\")\n",
    "        \n",
    "        # Load benign images (class 1)\n",
    "        benign_path = self.data_path / \"benign\"\n",
    "        if benign_path.exists():\n",
    "            benign_files = []\n",
    "            for ext in [\"*.jpeg\", \"*.jpg\", \"*.png\", \"*.JPEG\", \"*.JPG\", \"*.PNG\"]:\n",
    "                benign_files.extend(list(benign_path.glob(ext)))\n",
    "            \n",
    "            self.image_paths.extend(benign_files)\n",
    "            self.labels.extend([1] * len(benign_files))\n",
    "            print(f\"   Benign: {len(benign_files)} images\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Benign directory not found\")\n",
    "        \n",
    "        # Limit samples if specified\n",
    "        if max_samples and len(self.image_paths) > max_samples:\n",
    "            indices = random.sample(range(len(self.image_paths)), max_samples)\n",
    "            self.image_paths = [self.image_paths[i] for i in indices]\n",
    "            self.labels = [self.labels[i] for i in indices]\n",
    "        \n",
    "        print(f\"üìã Dataset loaded: {len(self.image_paths)} images\")\n",
    "        print(f\"   Adenocarcinoma: {self.labels.count(0)}\")\n",
    "        print(f\"   Benign: {self.labels.count(1)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading image {image_path}: {e}\")\n",
    "            image = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, str(image_path.name)\n",
    "\n",
    "# =============================================================================\n",
    "# üéØ ATTRIBUTION GENERATORS\n",
    "# =============================================================================\n",
    "\n",
    "class ScatNetAttributionHandler:\n",
    "    \"\"\"ScatNet attribution using perturbation analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, scatnet_model, device='cpu'):\n",
    "        self.model = scatnet_model\n",
    "        self.device = device\n",
    "        \n",
    "    def compute_perturbation_attribution(self, input_tensor, target_class=None, patch_size=8):\n",
    "        \"\"\"Compute attribution using occlusion analysis\"\"\"\n",
    "        print(f\"üîç Computing ScatNet attribution via perturbation analysis...\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        \n",
    "        # Get baseline prediction\n",
    "        with torch.no_grad():\n",
    "            baseline_output = self.model(input_tensor)\n",
    "            if target_class is None:\n",
    "                target_class = torch.argmax(baseline_output, dim=1).item()\n",
    "            baseline_score = baseline_output[0, target_class].item()\n",
    "        \n",
    "        # Initialize attribution map\n",
    "        _, c, h, w = input_tensor.shape\n",
    "        attribution_map = torch.zeros((1, c, h, w))\n",
    "        \n",
    "        # Sliding window perturbation\n",
    "        for i in range(0, h - patch_size + 1, patch_size // 2):\n",
    "            for j in range(0, w - patch_size + 1, patch_size // 2):\n",
    "                # Create perturbed input (zero out patch)\n",
    "                perturbed_input = input_tensor.clone()\n",
    "                perturbed_input[:, :, i:i+patch_size, j:j+patch_size] = 0\n",
    "                \n",
    "                # Get perturbed prediction\n",
    "                with torch.no_grad():\n",
    "                    perturbed_output = self.model(perturbed_input)\n",
    "                    perturbed_score = perturbed_output[0, target_class].item()\n",
    "                \n",
    "                # Attribution = baseline - perturbed (importance of removed patch)\n",
    "                importance = baseline_score - perturbed_score\n",
    "                attribution_map[:, :, i:i+patch_size, j:j+patch_size] += importance\n",
    "        \n",
    "        # Normalize attribution map\n",
    "        if torch.max(torch.abs(attribution_map)) > 0:\n",
    "            attribution_map = attribution_map / torch.max(torch.abs(attribution_map))\n",
    "        \n",
    "        print(f\"‚úÖ ScatNet perturbation attribution complete\")\n",
    "        return attribution_map\n",
    "\n",
    "class AttributionMapGenerator:\n",
    "    \"\"\"Main attribution map generator for both CNN and ScatNet\"\"\"\n",
    "    \n",
    "    def __init__(self, cnn_model, scatnet_model=None, device='cpu', explainability_path=None):\n",
    "        self.cnn_model = cnn_model\n",
    "        self.scatnet_model = scatnet_model\n",
    "        self.device = device\n",
    "        self.explainability_path = explainability_path\n",
    "        \n",
    "        # Initialize explainers\n",
    "        self.cnn_explainer = DeepLIFTFromScratch(cnn_model)\n",
    "        self.scatnet_handler = None\n",
    "        \n",
    "        if scatnet_model:\n",
    "            self.scatnet_handler = ScatNetAttributionHandler(scatnet_model, device)\n",
    "        \n",
    "        print(f\"üß† Attribution Map Generator initialized\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(f\"   CNN explainer: ‚úÖ DeepLIFT\")\n",
    "        print(f\"   ScatNet handler: {'‚úÖ Perturbation Analysis' if scatnet_model else '‚ùå Not provided'}\")\n",
    "    \n",
    "    def create_reference_inputs(self, input_tensor):\n",
    "        \"\"\"Create different types of reference inputs\"\"\"\n",
    "        references = {\n",
    "            'zero': torch.zeros_like(input_tensor),\n",
    "            'mean': torch.mean(input_tensor, dim=(2, 3), keepdim=True).expand_as(input_tensor),\n",
    "            'gaussian_blur': self._gaussian_blur(input_tensor),\n",
    "            'random_noise': torch.randn_like(input_tensor) * 0.1\n",
    "        }\n",
    "        return references\n",
    "    \n",
    "    def _gaussian_blur(self, tensor, kernel_size=15, sigma=3.0):\n",
    "        \"\"\"Apply Gaussian blur to tensor\"\"\"\n",
    "        try:\n",
    "            from torchvision.transforms.functional import gaussian_blur\n",
    "            if tensor.dim() == 4:\n",
    "                return torch.stack([gaussian_blur(img, kernel_size, sigma) for img in tensor])\n",
    "            else:\n",
    "                return gaussian_blur(tensor, kernel_size, sigma)\n",
    "        except:\n",
    "            # Fallback if gaussian_blur not available\n",
    "            return torch.zeros_like(tensor)\n",
    "    \n",
    "    def generate_attributions_for_sample(self, input_tensor, filename, target_class=None, save_individual=True):\n",
    "        \"\"\"Generate attribution maps for a single sample\"\"\"\n",
    "        \n",
    "        print(f\"\\nüîç Generating attributions for: {filename}\")\n",
    "        \n",
    "        # Move to device\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        \n",
    "        # Get CNN predictions\n",
    "        with torch.no_grad():\n",
    "            cnn_output = self.cnn_model(input_tensor)\n",
    "            cnn_pred = torch.softmax(cnn_output, dim=1)\n",
    "            cnn_class = torch.argmax(cnn_pred, dim=1).item()\n",
    "            cnn_confidence = cnn_pred[0, cnn_class].item()\n",
    "        \n",
    "        print(f\"   CNN prediction: Class {cnn_class} (confidence: {cnn_confidence:.3f})\")\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = cnn_class\n",
    "        \n",
    "        # Create reference inputs\n",
    "        references = self.create_reference_inputs(input_tensor)\n",
    "        \n",
    "        results = {\n",
    "            'filename': filename,\n",
    "            'input_shape': list(input_tensor.shape),\n",
    "            'target_class': target_class,\n",
    "            'cnn_prediction': {\n",
    "                'class': cnn_class,\n",
    "                'confidence': cnn_confidence\n",
    "            },\n",
    "            'attributions': {}\n",
    "        }\n",
    "        \n",
    "        # Generate CNN attributions with different references\n",
    "        for ref_name, reference in references.items():\n",
    "            print(f\"   üîç CNN attribution with {ref_name} reference...\")\n",
    "            \n",
    "            try:\n",
    "                self.cnn_explainer.set_reference(reference)\n",
    "                cnn_attr = self.cnn_explainer.compute_attributions(input_tensor, target_class)\n",
    "                \n",
    "                results['attributions'][f'cnn_{ref_name}'] = {\n",
    "                    'attribution_map': cnn_attr.cpu(),\n",
    "                    'attribution_sum': float(torch.sum(cnn_attr)),\n",
    "                    'attribution_norm': float(torch.norm(cnn_attr)),\n",
    "                    'positive_attribution': float(torch.sum(torch.clamp(cnn_attr, min=0))),\n",
    "                    'negative_attribution': float(torch.sum(torch.clamp(cnn_attr, max=0)))\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Error computing CNN attribution with {ref_name}: {e}\")\n",
    "        \n",
    "        # Generate ScatNet attributions if available\n",
    "        if self.scatnet_model and self.scatnet_handler:\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    scatnet_output = self.scatnet_model(input_tensor)\n",
    "                    scatnet_pred = torch.softmax(scatnet_output, dim=1)\n",
    "                    scatnet_class = torch.argmax(scatnet_pred, dim=1).item()\n",
    "                    scatnet_confidence = scatnet_pred[0, scatnet_class].item()\n",
    "                \n",
    "                results['scatnet_prediction'] = {\n",
    "                    'class': scatnet_class,\n",
    "                    'confidence': scatnet_confidence\n",
    "                }\n",
    "                \n",
    "                print(f\"   ScatNet prediction: Class {scatnet_class} (confidence: {scatnet_confidence:.3f})\")\n",
    "                \n",
    "                # Generate ScatNet attribution\n",
    "                scatnet_attr = self.scatnet_handler.compute_perturbation_attribution(\n",
    "                    input_tensor, target_class\n",
    "                )\n",
    "                \n",
    "                results['attributions']['scatnet_perturbation'] = {\n",
    "                    'attribution_map': scatnet_attr.cpu(),\n",
    "                    'attribution_sum': float(torch.sum(scatnet_attr)),\n",
    "                    'attribution_norm': float(torch.norm(scatnet_attr)),\n",
    "                    'positive_attribution': float(torch.sum(torch.clamp(scatnet_attr, min=0))),\n",
    "                    'negative_attribution': float(torch.sum(torch.clamp(scatnet_attr, max=0))),\n",
    "                    'method': 'perturbation_analysis'\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå Error computing ScatNet attribution: {e}\")\n",
    "        \n",
    "        # Save individual visualizations\n",
    "        if save_individual and self.explainability_path:\n",
    "            self._save_individual_attributions(input_tensor, results, filename)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _save_individual_attributions(self, input_tensor, results, filename):\n",
    "        \"\"\"Save individual attribution visualizations\"\"\"\n",
    "        \n",
    "        base_name = filename.replace('.jpeg', '').replace('.jpg', '').replace('.png', '')\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        attribution_maps_dir = self.explainability_path / \"attribution_maps\"\n",
    "        attribution_maps_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        saved_files = []\n",
    "        \n",
    "        for attr_name, attr_data in results['attributions'].items():\n",
    "            try:\n",
    "                attribution_map = attr_data['attribution_map']\n",
    "                \n",
    "                # Create visualization\n",
    "                fig = DeepLIFTVisualizer.plot_attribution_map(\n",
    "                    input_tensor,\n",
    "                    attribution_map,\n",
    "                    title=f\"{attr_name.upper()} Attribution - {base_name}\",\n",
    "                    save_path=None\n",
    "                )\n",
    "                \n",
    "                # Save figure\n",
    "                save_path = attribution_maps_dir / f\"{base_name}_{attr_name}.png\"\n",
    "                plt.savefig(str(save_path), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                if save_path.exists():\n",
    "                    saved_files.append(save_path.name)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error saving {attr_name}: {e}\")\n",
    "                plt.close()\n",
    "        \n",
    "        if saved_files:\n",
    "            print(f\"   üíæ Saved {len(saved_files)} attribution maps\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå No attribution maps were saved\")\n",
    "\n",
    "# =============================================================================\n",
    "# üöÄ MAIN ATTRIBUTION GENERATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main_attribution_generation():\n",
    "    \"\"\"Main function to generate attribution maps\"\"\"\n",
    "    \n",
    "    print(\"\\nüöÄ MAIN ATTRIBUTION MAP GENERATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Setup paths - you're in notebooks/04_explainability/\n",
    "    PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "    MODELS_PATH = PROJECT_ROOT / \"models\"\n",
    "    RESULTS_PATH = PROJECT_ROOT / \"results\"\n",
    "    EXPLAINABILITY_PATH = RESULTS_PATH / \"explainability\"\n",
    "    \n",
    "    print(f\"üìÅ Project paths:\")\n",
    "    print(f\"   Root: {PROJECT_ROOT}\")\n",
    "    print(f\"   Models: {MODELS_PATH}\")\n",
    "    print(f\"   Results: {RESULTS_PATH}\")\n",
    "    print(f\"   Explainability: {EXPLAINABILITY_PATH}\")\n",
    "    \n",
    "    # Create directories\n",
    "    RESULTS_PATH.mkdir(exist_ok=True)\n",
    "    EXPLAINABILITY_PATH.mkdir(exist_ok=True)\n",
    "    (EXPLAINABILITY_PATH / \"attribution_maps\").mkdir(exist_ok=True)\n",
    "    (EXPLAINABILITY_PATH / \"comparisons\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Verify paths\n",
    "    print(f\"\\nüîç Path verification:\")\n",
    "    print(f\"   Project root exists: {PROJECT_ROOT.exists()}\")\n",
    "    print(f\"   Models folder exists: {MODELS_PATH.exists()}\")\n",
    "    print(f\"   Results folder exists: {RESULTS_PATH.exists()}\")\n",
    "    \n",
    "    if not MODELS_PATH.exists():\n",
    "        print(f\"‚ùå Models directory not found: {MODELS_PATH}\")\n",
    "        return None\n",
    "    \n",
    "    # Find models\n",
    "    print(f\"\\nüîç Looking for models...\")\n",
    "    pth_files = list(MODELS_PATH.glob(\"*.pth\"))\n",
    "    print(f\"üìã Available .pth files: {[f.name for f in pth_files]}\")\n",
    "    \n",
    "    # Find CNN model\n",
    "    cnn_model_path = MODELS_PATH / \"best_cnn_model.pth\"\n",
    "    if not cnn_model_path.exists():\n",
    "        cnn_candidates = [f for f in pth_files if 'cnn' in f.name.lower()]\n",
    "        if cnn_candidates:\n",
    "            cnn_model_path = cnn_candidates[0]\n",
    "            print(f\"‚úÖ Using CNN model: {cnn_model_path.name}\")\n",
    "        else:\n",
    "            print(f\"‚ùå No CNN model found\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"‚úÖ CNN model found: {cnn_model_path.name}\")\n",
    "    \n",
    "    # Find ScatNet model\n",
    "    scatnet_model_path = MODELS_PATH / \"best_scatnet_model.pth\"\n",
    "    if not scatnet_model_path.exists():\n",
    "        scatnet_candidates = [f for f in pth_files if 'scatnet' in f.name.lower()]\n",
    "        if scatnet_candidates:\n",
    "            scatnet_model_path = scatnet_candidates[0]\n",
    "            print(f\"‚úÖ Using ScatNet model: {scatnet_model_path.name}\")\n",
    "        else:\n",
    "            print(f\"üìã No ScatNet model found - CNN attribution only\")\n",
    "            scatnet_model_path = None\n",
    "    else:\n",
    "        print(f\"‚úÖ ScatNet model found: {scatnet_model_path.name}\")\n",
    "    \n",
    "    # Find data\n",
    "    print(f\"\\nüìÅ Searching for data...\")\n",
    "    data_paths = [\n",
    "        PROJECT_ROOT / \"data\" / \"raw\",\n",
    "        PROJECT_ROOT / \"data\" / \"processed\",\n",
    "        PROJECT_ROOT / \"data\"\n",
    "    ]\n",
    "    \n",
    "    DATA_PATH = None\n",
    "    for data_path in data_paths:\n",
    "        if data_path.exists():\n",
    "            # Check for subdirectories with images\n",
    "            adeno_path = data_path / \"adenocarcinoma\"\n",
    "            benign_path = data_path / \"benign\"\n",
    "            if adeno_path.exists() or benign_path.exists():\n",
    "                DATA_PATH = data_path\n",
    "                print(f\"‚úÖ Data found at: {DATA_PATH}\")\n",
    "                break\n",
    "    \n",
    "    if DATA_PATH is None:\n",
    "        print(f\"‚ùå No data found, creating demo data...\")\n",
    "        DATA_PATH = PROJECT_ROOT / \"temp_demo_data\"\n",
    "        DATA_PATH.mkdir(exist_ok=True)\n",
    "        (DATA_PATH / \"adenocarcinoma\").mkdir(exist_ok=True)\n",
    "        (DATA_PATH / \"benign\").mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create demo images\n",
    "        for i in range(3):\n",
    "            img = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    "            Image.fromarray(img).save(DATA_PATH / \"adenocarcinoma\" / f\"demo_{i}.jpg\")\n",
    "            Image.fromarray(img).save(DATA_PATH / \"benign\" / f\"demo_{i}.jpg\")\n",
    "        \n",
    "        print(f\"‚úÖ Demo data created at: {DATA_PATH}\")\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"üîß Using device: {device}\")\n",
    "    \n",
    "    # Create simple models for demonstration\n",
    "    print(f\"\\nüì• Creating demo models...\")\n",
    "    \n",
    "    class SimpleCNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "            self.classifier = nn.Linear(32, 2)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.classifier(x)\n",
    "    \n",
    "    class SimpleScatNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, 7, stride=2, padding=3),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, 5, stride=2, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((1, 1))\n",
    "            )\n",
    "            self.classifier = nn.Linear(128, 2)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.features(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.classifier(x)\n",
    "    \n",
    "    # Create models\n",
    "    cnn_model = SimpleCNN().to(device)\n",
    "    cnn_model.eval()\n",
    "    print(f\"‚úÖ CNN demo model ready\")\n",
    "    \n",
    "    scatnet_model = None\n",
    "    if scatnet_model_path:\n",
    "        scatnet_model = SimpleScatNet().to(device)\n",
    "        scatnet_model.eval()\n",
    "        print(f\"‚úÖ ScatNet demo model ready\")\n",
    "    \n",
    "    # Setup data loading\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nüìÅ Loading dataset...\")\n",
    "    dataset = LungCancerDataset(DATA_PATH, transform=transform, max_samples=5)\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(f\"‚ùå No images found in dataset\")\n",
    "        return None\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # Initialize attribution generator\n",
    "    attr_generator = AttributionMapGenerator(\n",
    "        cnn_model, scatnet_model, device, EXPLAINABILITY_PATH\n",
    "    )\n",
    "    \n",
    "    # Generate attributions\n",
    "    print(f\"\\nüéØ Generating attribution maps...\")\n",
    "    all_results = []\n",
    "    \n",
    "    for i, (image, label, filename) in enumerate(dataloader):\n",
    "        if i >= 3:  # Limit to 3 samples\n",
    "            break\n",
    "        \n",
    "        print(f\"\\n--- Sample {i+1}/3 ---\")\n",
    "        try:\n",
    "            results = attr_generator.generate_attributions_for_sample(\n",
    "                image, filename[0], target_class=None, save_individual=True\n",
    "            )\n",
    "            all_results.append(results)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing sample {filename[0]}: {e}\")\n",
    "    \n",
    "    # Save results summary\n",
    "    if all_results:\n",
    "        results_summary = {\n",
    "            'attribution_generation': {\n",
    "                'status': 'completed',\n",
    "                'samples_processed': len(all_results),\n",
    "                'cnn_attributions': True,\n",
    "                'scatnet_attributions': scatnet_model is not None,\n",
    "                'save_path': str(EXPLAINABILITY_PATH)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save JSON summary\n",
    "        try:\n",
    "            results_file = EXPLAINABILITY_PATH / \"attribution_results_summary.json\"\n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(results_summary, f, indent=2)\n",
    "            print(f\"\\nüíæ Results summary saved: {results_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving summary: {e}\")\n",
    "        \n",
    "        # Verify saved files\n",
    "        print(f\"\\nüìÇ Verifying saved files:\")\n",
    "        attribution_maps_dir = EXPLAINABILITY_PATH / \"attribution_maps\"\n",
    "        if attribution_maps_dir.exists():\n",
    "            png_files = list(attribution_maps_dir.glob(\"*.png\"))\n",
    "            print(f\"   Attribution maps: {len(png_files)} files\")\n",
    "            for png_file in png_files[:5]:  # Show first 5\n",
    "                print(f\"     - {png_file.name}\")\n",
    "        \n",
    "        print(f\"\\nüéâ ATTRIBUTION MAP GENERATION COMPLETE!\")\n",
    "        print(f\"üìä Summary:\")\n",
    "        print(f\"   ‚Ä¢ Samples processed: {len(all_results)}\")\n",
    "        print(f\"   ‚Ä¢ CNN attributions: ‚úÖ\")\n",
    "        print(f\"   ‚Ä¢ ScatNet attributions: {'‚úÖ' if scatnet_model else '‚ùå'}\")\n",
    "        print(f\"   ‚Ä¢ Files saved to: {EXPLAINABILITY_PATH}\")\n",
    "        \n",
    "        return results_summary\n",
    "    \n",
    "    else:\n",
    "        print(f\"‚ùå No samples processed successfully\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# üöÄ EXECUTE ATTRIBUTION GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã ATTRIBUTION MAPS GENERATION: READY\")\n",
    "print(\"üöÄ Starting attribution generation...\")\n",
    "\n",
    "# Run the main function\n",
    "try:\n",
    "    results = main_attribution_generation()\n",
    "    if results:\n",
    "        print(\"\\nüéâ Attribution generation completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Attribution generation encountered issues\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during attribution generation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã SCRIPT COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
